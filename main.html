<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Yash Savani Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213 ysavani@cs.cmu.edu" />
  <title>Advanced Foundations of Machine Learning</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Advanced Foundations of Machine Learning</h1>
<p class="author">Yash Savani<br />
Computer Science Department<br />
Carnegie Mellon University<br />
Pittsburgh, PA 15213<br />
ysavani@cs.cmu.edu</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#meta-tools">Meta Tools</a></li>
<li><a href="#core-math">Core Math</a>
<ul>
<li><a href="#problem-solving-entry-attack-review">Problem Solving (Entry, Attack, Review)</a></li>
<li><a href="#propositional-logic">Propositional Logic</a></li>
<li><a href="#set-theory-zfc-axioms">Set Theory (ZFC axioms)</a></li>
<li><a href="#predicate-logic">Predicate Logic</a></li>
<li><a href="#proof-techniques">Proof Techniques</a></li>
<li><a href="#relations">Relations</a></li>
<li><a href="#functions">Functions</a></li>
<li><a href="#number-systems">Number Systems</a></li>
<li><a href="#basic-number-theory">Basic Number Theory</a></li>
<li><a href="#modular-arithmetic">Modular Arithmetic</a></li>
<li><a href="#birkhoff-geometry-geometry-based-on-real-numbers">Birkhoff Geometry (geometry based on real numbers)</a></li>
<li><a href="#systems-of-equations-matrices-polynomials">Systems of Equations, Matrices, Polynomials</a></li>
<li><a href="#group-theory">Group Theory</a></li>
<li><a href="#rings-and-fields">Rings and Fields</a></li>
<li><a href="#morphisms">Morphisms</a></li>
<li><a href="#single-variable-calculus">Single Variable Calculus</a></li>
</ul></li>
<li><a href="#core-cs">Core CS</a>
<ul>
<li><a href="#programming-basics">Programming basics</a></li>
<li><a href="#algorithms">Algorithms</a></li>
<li><a href="#data-structures">Data Structures</a></li>
<li><a href="#systems-and-additional-topics">Systems and Additional Topics</a></li>
</ul></li>
<li><a href="#intermediate-math">Intermediate Math</a>
<ul>
<li><a href="#linear-algebra">Linear Algebra</a></li>
<li><a href="#real-analysis">Real Analysis</a></li>
<li><a href="#multivariable-calculus-using-analysis">Multivariable Calculus Using Analysis</a></li>
<li><a href="#probability-theory-measure-theory-and-stochastic-processes">Probability Theory (Measure Theory and Stochastic Processes)</a></li>
<li><a href="#statistics">Statistics</a></li>
</ul></li>
<li><a href="#numerical-methods-and-optimization">Numerical Methods and Optimization</a></li>
<li><a href="#core-ml">Core ML</a></li>
<li><a href="#seminal-research-in-ml">Seminal Research in ML</a></li>
<li><a href="#tools">Tools</a></li>
<li><a href="#additional-topics">Additional Topics</a></li>
<li><a href="#sec:acknowledgements">Acknowledgements</a></li>
</ul>
</nav>
<h1 id="introduction">Introduction</h1>
<p>There are already several repositories and courses that cover the foundations of Machine Learning. Here are a few of them:</p>
<ul>
<li><p>https://github.com/jonkrohn/ML-foundations, and</p></li>
<li><p>https://github.com/dontless/Machine-Learning-Foundations-A-Case-Study-Approach.</p></li>
</ul>
<p>Most of these resources are designed to get you to the starting line. They provide just enough material for you to get a job doing machine learning, or for you to get started on machine learning projects. However, if you have ever tried reading a theoretical machine learning paper from a conference like <a href="http://www.learningtheory.org/colt2021/accepted.html">COLT</a> or one of the more advanced applied papers from a conference like <a href="https://icml.cc/Conferences/2021/Schedule?type=Poster">ICML</a>, it is usually clear that a lot is missing from these “foundational” curricula.</p>
<p>Some might argue that the best way to learn this missing material is to just dive in and start reading papers. Try to figure out the missing concepts on the fly. That is exactly what I have been doing over the past few years, and I am sad to say that this approach is very insufficient. While it may get you to a place where you can understand the research superficially, this top-down model leaves you with a deep sense of inadequacy; a feeling that you are lacking something base. With this approach, you eventually realize that you lack a cohesive narrative and many foundational concepts. You find it difficult to see the work in a wider theoretical and applied context, and have a hard time identifying nontrivial extensions. The need to go back and relearn some of the mathematical foundations then becomes imperative. This is exactly what I went through, and if you have had a similar experience, I hope this resource will be of some help to you.</p>
<p>If you have no experience with machine learning at all, then I would encourage you to take an intro class in machine learning to see just how cool this field is. One example of such a course is (https://www.coursera.org/learn/machine-learning) by Andrew Ng. Machine learning is one of the most exciting fields to have blossomed over the past decade, and there are still a plethora of untapped applications to many of the existing techniques. I urge you to explore as much as you can, and also to start reading papers and thinking of extensions to contemporary work. This material should be seen more as a complement than a prerequisite to your machine learning journey. If you find yourself struggling with some of the advanced material, or you get a gnawing sense that you are missing something fundamental while reading the literature, then maybe something from this resource can help you.</p>
<p>For those of you who already have some experience with machine learning, I hope this resource can act as a good reference and review checklist. There are many topics and perspectives to consider any machine learning problem through. While you may be familiar with some of the ideas listed here, I hope this repository presents a few alternative viewpoints that you may have neglected to consider your problems through. Hopefully, these alternative lenses can highlight some key, deep insights into your problem that would otherwise have lied dormant. Furthermore, having a systematic checklist for review items can also help focus and prioritize future explorations. By going through the topics listed here, you can identify areas of high impact that you may not completely comfortable with. Focusing on these areas has the potential to maximize the utility of your explorations.</p>
<p>My goal with this repository is to create a curated list of resources for those who want to do a deeper dive into some of the more advanced foundations of machine learning. I hope that anyone with a high school background in math and CS who goes through all the material here will successfully be able to read and carry out state-of-the-art research in both theoretical and applied machine learning feeling empowered and confident in their work.</p>
<p>The rest of this document is organized into several different sub-fields, with accompanying learning resources, that I think are necessary to gain this advanced foundation in machine learning.</p>
<h1 id="meta-tools">Meta Tools</h1>
<p>Before you start this journey, I would recommend making sure you have a solid arsenal of meta tools that will expedite your acquisition and integration of the new concepts you will be learning. These are tools like a good code and manuscript editor (I use neovim with plugins. See my <a href="https://github.com/yashsavani/dotfiles">dotfiles</a> for my configuration), a good reference manager (I use Zotero, but I’ve heard good things about Mendeley as well), and a good concept management system (I use Zettelkasten: for an introduction to this system look at <a href="https://zenkit.com/en/blog/a-beginners-guide-to-the-zettelkasten-method/">this blog post</a>. For a deeper dive, read <a href="https://www.amazon.com/How-Take-Smart-Notes-Nonfiction/dp/1542866502">How to Take Smart Notes</a>).</p>
<p>I also generally prefer videos over textbooks to learn new material. Unfortunately, the speed of any video tends to vary a lot and does not necessarily match my comfort level with the material. I use the <a href="https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk">Video Speed Controller</a> chrome extension to dial in the speed I want.</p>
<h1 id="core-math">Core Math</h1>
<p>Before diving into some of the more advanced math concepts, I would encourage you to at least cursorily go through some of the core math resources provided here. I recommend doing this even if you think you are comfortable with all the topics listed below. I can’t count the number of times I thought I completely understood some core math concept, only to later realize that there was some subtlety I had neglected. These neglected subtleties can often have a cascading effect making it very hard to understand some of the more advanced material that rely on a solid core.</p>
<h2 id="problem-solving-entry-attack-review">Problem Solving (Entry, Attack, Review)</h2>
<ul>
<li><p>Problems are ones that:</p>
<ul>
<li><p>Engage intellect</p></li>
<li><p>Make connections to develop a coherent framework</p></li>
<li><p>Can be solved in more than one way</p></li>
<li><p>Should foster effective communication of mathematical ideas</p></li>
</ul></li>
<li><p>Phases of Problem Solving:</p>
<ul>
<li><p>Entry: what do I know (question, experience), what do I want (paraphrase, ambiguities), what can I introduce (diagram, notation).</p></li>
<li><p>Attack: brute force, look for patterns.</p></li>
<li><p>Review: check, reflect, extend, understand why it works.</p></li>
</ul></li>
</ul>
<h2 id="propositional-logic">Propositional Logic</h2>
<ul>
<li><p>Propositions,</p></li>
<li><p>Connectives (negation, conjunction, inclusive and exclusive disjunction),</p></li>
<li><p>De Morgan’s theorem,</p></li>
<li><p>Implication (converse, inverse, contrapositive),</p></li>
<li><p>Biconditional,</p></li>
<li><p>Truth tables, and</p></li>
<li><p>Logical equivalence.</p></li>
</ul>
<h2 id="set-theory-zfc-axioms">Set Theory (ZFC axioms)</h2>
<ul>
<li><p>Roster and set builder notation,</p></li>
<li><p>Sets contain elements,</p></li>
<li><p>Operations (intersection <span>[</span>conjunction<span>]</span>, union <span>[</span>disjunction<span>]</span>, difference, symmetric difference),</p></li>
<li><p>universal and null/empty set,</p></li>
<li><p>Complement,</p></li>
<li><p>Venn diagrams,</p></li>
<li><p>Subset (implication) and superset,</p></li>
<li><p>Equality (biconditional), and</p></li>
<li><p>Family of sets and Russell’s paradox.</p></li>
</ul>
<h2 id="predicate-logic">Predicate Logic</h2>
<ul>
<li><p>Noun phrase (subject) and verb phrase (predicate),</p></li>
<li><p>Truth set,</p></li>
<li><p>Universal quantifier,</p></li>
<li><p>Existential quantifier,</p></li>
<li><p>Unique existential quantifier,</p></li>
<li><p>Compound quantifiers,</p></li>
<li><p>Negation of quantifiers, and</p></li>
<li><p>Index sets (union and intersection).</p></li>
</ul>
<h2 id="proof-techniques">Proof Techniques</h2>
<ul>
<li><p>Axioms/postulates, conjectures, lemmas, theorems, corollaries,</p></li>
<li><p>Completeness, consistency, and decidability (Godel’s incompleteness theorems),</p></li>
<li><p>Valid arguments (modus ponens, modus tollens, law of syllogism),</p></li>
<li><p>Invalid arguments (affirming the consequent),</p></li>
<li><p>Direct proof,</p></li>
<li><p>Proof by contrapositive,</p></li>
<li><p>Proof by contradiction,</p></li>
<li><p>Proof of conjunction,</p></li>
<li><p>Proof of inclusive disjunction,</p></li>
<li><p>Universal: let x be arbitrary, prove p(x),</p></li>
<li><p>Existential: find an x such that p(x) is true,</p></li>
<li><p>Uniqueness: assume p(x), p(y), show x = y,</p></li>
<li><p>Successor operation,</p></li>
<li><p>Proof by induction (weak and strong), and</p></li>
<li><p>Deductive vs inductive reasoning.</p></li>
</ul>
<h2 id="relations">Relations</h2>
<ul>
<li><p>Tuples,</p></li>
<li><p>Cartesian products (cartesian square, cartesian power),</p></li>
<li><p>Arity of operations,</p></li>
<li><p>Domain and codomain,</p></li>
<li><p>Inverse relations,</p></li>
<li><p>Graphs and digraphs,</p></li>
<li><p>Relations on a set,</p></li>
<li><p>Pre-order relations (reflexive, transitive),</p></li>
<li><p>Equivalence relations (reflexive, transitive, symmetric),</p></li>
<li><p>Equivalence classes,</p></li>
<li><p>Partial-order relations (reflexive, transitive, antisymmetric) (posets),</p></li>
<li><p>Hasse diagrams,</p></li>
<li><p>Total-order relations (complete, transitive, antisymmetric),</p></li>
<li><p>Strict-order relations (reflexive -&gt; irreflexive) (trichotomous),</p></li>
<li><p>Upper bounds, lower bounds, maximum, minimum, and</p></li>
<li><p>Least upper bound (supremum), greatest lower bound (infimum).</p></li>
</ul>
<h2 id="functions">Functions</h2>
<ul>
<li><p>Left-total, left-unique, right-total, right-unique,</p></li>
<li><p>Function (left-total and right-unique),</p></li>
<li><p>Prototype and definition,</p></li>
<li><p>Images, ranges, and preimages,</p></li>
<li><p>Injective/one-to-one (left-unique),</p></li>
<li><p>Surjective/onto (right-total),</p></li>
<li><p>Bijective/one-to-one correspondence (right-total and left-unique),</p></li>
<li><p>Inverse functions,</p></li>
<li><p>Function composition,</p></li>
<li><p>Restricing domain and codomain, and</p></li>
<li><p>Monotonicity.</p></li>
</ul>
<h2 id="number-systems">Number Systems</h2>
<ul>
<li><p>Cardinality,</p></li>
<li><p>Pigeon-hole principle,</p></li>
<li><p>Cantor-Bernstein-Schroder theorem,</p></li>
<li><p>Finite Sets,</p></li>
<li><p>Peano’s axioms,</p></li>
<li><p>Countable sets (Aleph-Null),</p></li>
<li><p>Integers,</p></li>
<li><p>Rationals (snaking argument),</p></li>
<li><p>Arithmetic operations,</p>
<ul>
<li><p>Addition with inverse (subtraction),</p></li>
<li><p>Multiplication (repeated addition) with inverse (division),</p></li>
</ul></li>
<li><p>Exponentiation (repeated multiplication) with inverses (logarithms and radicals/roots since exponentiation is not commutative),</p></li>
<li><p>Algebraic numbers,</p></li>
<li><p>Real Numbers</p>
<ul>
<li><p>Zenos’ paradoxes,</p></li>
<li><p>Transcendental numbers,</p></li>
<li><p>Dedekind cut,</p></li>
<li><p>Supremum property, and</p></li>
<li><p>Cantor’s diagonalization argument,</p></li>
</ul></li>
<li><p>Cardinality of the continuum (continuum hypothesis),</p></li>
<li><p>Hilbert’s paradox of the Grand Hotel,</p></li>
<li><p>Complex numbers (closure under roots) (no ordering),</p>
<ul>
<li><p>Complex arithmetic,</p></li>
<li><p>Conjugate,</p></li>
<li><p>Modulus,</p></li>
<li><p>Argand diagram, and</p></li>
</ul></li>
<li><p>Quarternions (not commutative) and octonions (not associative).</p></li>
</ul>
<h2 id="basic-number-theory">Basic Number Theory</h2>
<ul>
<li><p>Divisibility,</p></li>
<li><p>Multiples and factors,</p></li>
<li><p>Euclidean division (dividend = quotient divisor + remainder),</p></li>
<li><p>Greatest common divisor, and lowest common multiple,</p></li>
<li><p>Relative primes,</p></li>
<li><p>Euclid’s algorithm,</p></li>
<li><p>Linear combination,</p></li>
<li><p>Diophantine equations,</p></li>
<li><p>Prime numbers,</p></li>
<li><p>Unique prime factorization,</p></li>
<li><p>Euler totient function,</p></li>
<li><p>Prime counting function,</p></li>
<li><p>Sieve of Eratosthenes,</p></li>
<li><p>Mersenne primes,</p></li>
<li><p>Mill’s theorem, and</p></li>
<li><p>Riemann Hypothesis.</p></li>
</ul>
<h2 id="modular-arithmetic">Modular Arithmetic</h2>
<ul>
<li><p>Remainders follow cyclic pattern,</p></li>
<li><p>Congruence modulo m,</p></li>
<li><p>Properties of congruence,</p></li>
<li><p>Congruence classes,</p></li>
<li><p>Cancelling when dividing by relative prime of modulus,</p></li>
<li><p>Divisibility tests, and</p></li>
<li><p>Multiplicative inverses.</p></li>
</ul>
<h2 id="birkhoff-geometry-geometry-based-on-real-numbers">Birkhoff Geometry (geometry based on real numbers)</h2>
<ul>
<li><p>Undefined terms</p>
<ul>
<li><p>point,</p></li>
<li><p>line (set of points),</p></li>
<li><p>distance (positive definite, symmetric, triangle inequality), and</p></li>
<li><p>angle (formed by 3 points).</p></li>
</ul></li>
<li><p>Definitions</p>
<ul>
<li><p>Between,</p></li>
<li><p>Line segment,</p></li>
<li><p>Half-line/ray and endpoint,</p></li>
<li><p>Parallel,</p></li>
<li><p>Straight angle, right-angle, perpendicular,</p></li>
<li><p>Triangle, vertices, degenerate triangle, and</p></li>
<li><p>Similar and congruent.</p></li>
</ul></li>
<li><p>Postulates</p>
<ul>
<li><p>Bijection between points of a line and real numbers,</p></li>
<li><p>Unique line containing two distinct points,</p></li>
<li><p>Bijection between rays and real numbers mod 2 pi, and</p></li>
<li><p>Triangle congruences.</p></li>
</ul></li>
<li><p>Theorems</p>
<ul>
<li><p>Linear pair,</p></li>
<li><p>Vertical angles,</p></li>
<li><p>Triangle angles sum,</p></li>
<li><p>Corresponding angles (and converse),</p></li>
<li><p>Alternate interior/exterior angles (and converses),</p></li>
<li><p>Interiors on the same side (and converse),</p></li>
<li><p>Perimeter and Area,</p></li>
<li><p>Parallelogram,</p></li>
<li><p>Rectangle,</p></li>
<li><p>Rhombus,</p></li>
<li><p>Square,</p></li>
<li><p>Trapezoid,</p></li>
<li><p>Polygons (regular and irregular),</p></li>
<li><p>Circles (radius and diameter, chords, tangents, arcs, angles),</p></li>
</ul></li>
<li><p>3D cylinders and volume,</p></li>
<li><p>Planes and disks,</p></li>
<li><p>Spheres,</p></li>
<li><p>Polyhedrons are 3D object with flat faces and edges, and</p></li>
<li><p>Dihedrons are a 2D object in 3D space with two faces and edges of neglegible thickness.</p></li>
<li><p>Trigonometry</p>
<ul>
<li><p>Right-angle triangle (opposite, adjacent, hypotenuse),</p></li>
<li><p>Sine, cosine, tangent,</p></li>
<li><p>Reciprocals (cosecant, secant, cotangent),</p></li>
<li><p>Inverses (arcsine, arccosine, arctangent)</p></li>
<li><p>Tan = sin / cos,</p></li>
<li><p>Unit circle,</p></li>
<li><p>Radians vs degrees,</p></li>
<li><p>Domains and ranges,</p></li>
<li><p>Periods,</p></li>
<li><p>Pythagorean identities (<span class="math inline">\(\sin^2 + \cos^2 = 1\)</span>)</p></li>
<li><p>Sine is odd, cos is even,</p></li>
<li><p>Sum and difference formulae (cos together and flip, sin seperate and keep),</p></li>
<li><p>Product to sum and sum to product formulae,</p></li>
<li><p>Convert sine to cosine by phase shift,</p></li>
<li><p>Relation to complex numbers (<span class="math inline">\(e^{ix} = \cos(x) + i \cdot \sin(x)\)</span>),</p></li>
<li><p>Add complex numbers using parallelogram rule, and</p></li>
<li><p>Multiply complex numbers by adding the angles and multiplying the lengths.</p></li>
</ul></li>
</ul>
<h2 id="systems-of-equations-matrices-polynomials">Systems of Equations, Matrices, Polynomials</h2>
<ul>
<li><p>Algebraic expressions (rational exponents),</p></li>
<li><p>Linear equations and inequalities,</p></li>
<li><p>Linear graphs,</p></li>
<li><p>Critical points,</p></li>
<li><p>Multiple equations with multiple unknowns,</p></li>
<li><p>Writing linear systems of equations as matrices,</p></li>
<li><p>Matrix multiplication,</p></li>
<li><p>Matrix inverses (singular matrices),</p></li>
<li><p>Absolute value and piecewise equations,</p></li>
<li><p>Monomials,</p></li>
<li><p>Polynomials,</p></li>
<li><p>Quadratic equation,</p></li>
<li><p>Cubic equation,</p></li>
<li><p>No closed form for quartic equation (Galois),</p></li>
<li><p>Binomials,</p></li>
<li><p>Binomial theorem (permutations and combinations) (Pascal’s triangle),</p>
<ul>
<li><p><span class="math inline">\((a-b)^2 = (a-b)(a+b)\)</span>,</p></li>
</ul></li>
<li><p>Foil,</p></li>
<li><p>Polynomial arithmetic,</p></li>
<li><p>Polynomial factorization,</p></li>
<li><p>Polynomial division,</p></li>
<li><p>Partial fractions (Partial fraction decomposition),</p></li>
<li><p>Polynomial graphs,</p></li>
<li><p>Reciprocal graph,</p></li>
<li><p>Complex polynomials, and</p></li>
<li><p>Fundamental theorem of algebra (any polynomial of degree d has d, possibly complex, roots) (connects algebra with geometry).</p></li>
</ul>
<h2 id="group-theory">Group Theory</h2>
<ul>
<li><p>Group (closed, associative, unique identity, unique inverse),</p></li>
<li><p>Idempotence,</p></li>
<li><p>Order of a group is the cardinality of the group,</p></li>
<li><p>Finite and infinite groups,</p></li>
<li><p>GLnR group of nonsingular matrices under matrix multiplication,</p></li>
<li><p>Abelian groups (commutative group),</p></li>
<li><p>Cayley table,</p></li>
<li><p>Subgroup (show closure and inverse for infinite group, only closure for finite group),</p></li>
<li><p>Exponentiation,</p></li>
<li><p>Generated group,</p></li>
<li><p>Order of an element,</p></li>
<li><p>Cyclic group,</p></li>
<li><p>Dihedral group (symmetries of a regular polygon), and</p></li>
<li><p>Symmetric/permutation group (bijections from a set to itself) (transpositions, cycles, compostion).</p></li>
</ul>
<h2 id="rings-and-fields">Rings and Fields</h2>
<ul>
<li><p>Rings are additive abelian groups that are closed under multiplication,</p>
<ul>
<li><p>Addition and multiplication are linked through the distributive property,</p></li>
</ul></li>
<li><p>Exponentiation is repeated multiplication,</p></li>
<li><p>0a = a0 = 0,</p></li>
<li><p>a (-b) = (-a) b = -(ab),</p></li>
<li><p>-(-a)(-b) = ab,</p></li>
<li><p>Infinite and finite rings,</p></li>
<li><p>Polynomial rings,</p></li>
<li><p>Commutative rings,</p></li>
<li><p>Unital ring (multiplicative identity) (inverses and units),</p>
<ul>
<li><p>1 = 0 implies the ring is the trivial ring,</p></li>
</ul></li>
<li><p>Division ring (unital ring where every non-zero element is a unit),</p></li>
<li><p>Zero divisor (non-zero element times another non-zero element gives 0),</p></li>
<li><p>Intersection of units and zero divisors is empty,</p></li>
<li><p>Integral domain (commutative, unital ring with no zero divisors),</p></li>
<li><p>Can cancel factors from both sides of an integral domain,</p></li>
<li><p>A field is an algebraic structure that is abelian under both addition and multiplication,</p>
<ul>
<li><p>A field is an integral domain where every non-zero element is a unit,</p></li>
</ul></li>
<li><p>Rationals, reals, and complex numbers are infinite fields,</p></li>
<li><p>Integers modulo a prime are finite fields, and</p></li>
<li><p>All arithmetic operations <span class="math inline">\(+, -, \times\)</span>, and <span class="math inline">\(\div\)</span> play nicely together.</p></li>
</ul>
<h2 id="morphisms">Morphisms</h2>
<ul>
<li><p>Symmetry is patterned self-similarity,</p></li>
<li><p>Dihedral group <span class="math inline">\(D_n\)</span> is a non-abelian, non-cyclic group with order <span class="math inline">\(2n\)</span>,</p></li>
<li><p>Homomorphisms (operation preserving functions),</p></li>
<li><p>Isomorphisms (bijective homomorphisms) (equivalence relation),</p></li>
<li><p>Endomorphisms and automorphisms (isomorphic endomorphism),</p></li>
<li><p>Automorphism group,</p></li>
<li><p>Group action,</p></li>
<li><p>Any finite cyclic group of order m is isomorphic to the integers mod m.</p></li>
<li><p>Cayley’s theorem (every group is isomorphic to a subgroup of the symmetric group).</p></li>
<li><p>Every permutation can be expressed as a composition of transpositions, and</p></li>
<li><p>Every permutation has a fixed parity (odd or even based on number of transpositions).</p></li>
</ul>
<h2 id="single-variable-calculus">Single Variable Calculus</h2>
<ul>
<li><p>Zenos’ paradoxes,</p></li>
<li><p>Sequences (notation, inductive and recursive definition),</p></li>
<li><p>Limits (intuitive definition),</p>
<ul>
<li><p>Limits at infinity and infinite limits,</p></li>
<li><p>One sided limits,</p></li>
</ul></li>
<li><p>Convergence and divergence (periodic),</p></li>
<li><p>Squeeze theorem,</p></li>
<li><p>Properties of limits,</p>
<ul>
<li><p>Constant scaling,</p></li>
<li><p>Closure under arithmetic operations (<span class="math inline">\(+, -, \times, \div\)</span>, powers, and roots),</p></li>
</ul></li>
<li><p>Limit evaluation,</p></li>
<li><p>Continuous functions and closure of contiuity under composition,</p></li>
<li><p>Intermediate value theorem,</p></li>
<li><p>Derivative,</p>
<ul>
<li><p>Interpretation as relative changes in variables,</p></li>
</ul></li>
<li><p>L’Hopital’s rule,</p></li>
<li><p>Polynomial derivative,</p></li>
<li><p>Basic properties of derivatives,</p>
<ul>
<li><p>Constant derivative is zero,</p></li>
<li><p>Constant scaling,</p></li>
<li><p>Addition, subtraction,</p></li>
<li><p>Product rule,</p></li>
<li><p>Quotient rule,</p></li>
<li><p>Power rule,</p></li>
<li><p>Chain rule,</p></li>
</ul></li>
<li><p>Exponential derivative and Euler’s number,</p></li>
<li><p>Higher order derivatives (smooth functions),</p></li>
<li><p>Implicit differentiation (logarthmic derivative),</p></li>
<li><p>Common derivatives,</p>
<ul>
<li><p>c, x, sin, cos, tan, arcsin, arccos, arctan, e^x, a^x, ln(x),</p></li>
</ul></li>
<li><p>Optimization (constrained and unconstrained),</p></li>
<li><p>Critical points and extrema (local and global),</p></li>
<li><p>Convex and concave functions,</p></li>
<li><p>Extreme value theorem,</p></li>
<li><p>1st and 2nd derivative test,</p></li>
<li><p>Lagrange multipliers,</p></li>
<li><p>Newton’s method,</p></li>
<li><p>Mean value theorem,</p></li>
<li><p>Riemann integral and anti-derivative,</p></li>
<li><p>Fundamental theorem of calculus,</p></li>
<li><p>Definite vs indefinite integral,</p></li>
<li><p>Properties of anti-derivative,</p>
<ul>
<li><p>Constant scaling,</p></li>
<li><p>Addition and subtraction,</p></li>
</ul></li>
<li><p>Common integrals,</p>
<ul>
<li><p>k, polynomials, 1/x, ln(x), e^x, sin, cos, tan</p></li>
</ul></li>
<li><p>Standard integral techniques,</p>
<ul>
<li><p>Substitution,</p></li>
<li><p>Integration by parts,</p></li>
</ul></li>
<li><p>Partial sums and infinite series,</p>
<ul>
<li><p>Absolutely convergence,</p></li>
<li><p>Conditionally convergence,</p></li>
</ul></li>
<li><p>Arithmetic series (average of the terms times the number of terms),</p></li>
<li><p>Geometric series (infinite limit = a/(1-r)),</p></li>
<li><p>Telescoping series,</p></li>
<li><p>Harmonic series (infinitely divergent) and p-series,</p></li>
<li><p>Convergence tests,</p>
<ul>
<li><p>Integral test,</p></li>
<li><p>(Limit) comparison test,</p></li>
<li><p>Alternating series test,</p></li>
<li><p>Ratio test,</p></li>
<li><p>Root test,</p></li>
<li><p>Power series (radius of convergence), and</p></li>
</ul></li>
<li><p>Taylor series,</p>
<ul>
<li><p>1/(1-x) = sum x^i,</p></li>
<li><p>e^x = sum x^i / i!,</p></li>
<li><p>cos x = sum (-1)^n x^{2n}/(2n)!,</p></li>
<li><p>sin x = sum (-1)^n x^{2n+1}/(2n+1)!.</p></li>
</ul></li>
</ul>
<p>One excellent resource I have found for some of this material is the playlist <a href="https://youtube.com/playlist?list=PLZzHxk_TPOStgPtqRZ6KzmkUQBQ8TSWVX">Introduction to Higher Mathematics</a>. I recommend going through this playlist at least once.</p>
<p>A crucial tool in the machine learning toolbox is calculus. Most of the machine learning literature makes extensive use of multivariable calculus. However, before diving into the nuances of multivariable calculus, it’s important to make sure you have a solid foundation and intuition for single variable calculus first. To gain the intuition I would recommend a <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">this playlist</a> by 3Blue1Brown, or <a href="https://www.khanacademy.org/math/ap-calculus-bc">this course</a> on AP calculus from Khan Academy. If you already have a strong calculus foundation, you can safely skip this section. Though, I would still encourage you to go through <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">the essence of calculus playlist</a> by 3Blue1Brown at some point though. It helps fill in a lot of intuition that more formal classes may have missed. Who knows? You may learn something you didn’t realize you had missed. Even small insights in the foundations can lead to major revelations later on.</p>
<h1 id="core-cs">Core CS</h1>
<p>Many of the recent innovations in machine learning can be traced directly back to computer science roots. Furthermore, much of the language used in the contemporary machine learning literature is inherited almost entirely from the computer science literature. Also, ultimately most machine learning ideas will have to be transcribed into code that will run on computers. Therefore, having a solid foundation in computer science will be imperitave to your success in machine learning research. All the topics covered here should be part of any introductory CS curriculum.</p>
<h2 id="programming-basics">Programming basics</h2>
<ul>
<li><p>Types,</p></li>
<li><p>Variables,</p></li>
<li><p>Arrays and strings,</p></li>
<li><p>Conditionals,</p></li>
<li><p>Loops,</p></li>
<li><p>Functions,</p></li>
<li><p>Bits and bitwise operations,</p></li>
<li><p>Recursion and backtracking,</p></li>
<li><p>Classes and objects,</p></li>
<li><p>Functional vs object oriented paradigms, and</p></li>
<li><p>Pointers/references and memory organization (stack and heap).</p></li>
</ul>
<h2 id="algorithms">Algorithms</h2>
<ul>
<li><p>Search,</p></li>
<li><p>Sort,</p></li>
<li><p>Divide and Conquer,</p></li>
<li><p>Bachmann-Landau/asymptotic notation (Big/Little O, Omega, Theta),</p></li>
<li><p>Master theorem,</p></li>
<li><p>Randomized algorithms,</p></li>
<li><p>Graphs/networks,</p></li>
<li><p>Minimum spanning trees,</p></li>
<li><p>Balancing trees,</p></li>
<li><p>Depth first search and breadth first search,</p></li>
<li><p>Dijkstra,</p></li>
<li><p>Dynamic programming (Bellman Ford, Floyd Warshall, Knapsack),</p></li>
<li><p>Min-cut and max-flow (Ford-Fulkerson), and</p></li>
<li><p>P vs NP (NP-completeness, 3 SAT, reductions).</p></li>
</ul>
<h2 id="data-structures">Data Structures</h2>
<ul>
<li><p>Lists,</p></li>
<li><p>Stacks,</p></li>
<li><p>Queues (priority queues),</p></li>
<li><p>Trees,</p></li>
<li><p>Heaps,</p></li>
<li><p>Hashmaps (amortization), and</p></li>
<li><p>Sets.</p></li>
</ul>
<h2 id="systems-and-additional-topics">Systems and Additional Topics</h2>
<ul>
<li><p>Hardware overview (transistors, logic gates, latches, memory, CPU, RAM, GPU, peripherals),</p></li>
<li><p>Unix (filesystem, interrupts/signals, system calls, processess, interprocess communication, terminal, shell scripting),</p></li>
<li><p>Multiprocessing vs multithreading, and asynchronous processing (mutexes, semaphores, threadpools, race conditions, deadlocks),</p></li>
<li><p>Networking (TCP/IP, sockets, HTTP, HTML5, CSS3, bandwidth, latency, throughput),</p></li>
<li><p>Standard libraries (string manipulation, file I/O, datetime, basic arithmetic and math operations),</p></li>
<li><p>Regular expressions,</p></li>
<li><p>Debugging techniques, and</p></li>
<li><p>Profiling.</p></li>
</ul>
<p>I learned most of this material through various online and in person classes over 13 years ago. As a result, most of the sources I used have since become antiquated and have been replaced with several better and friendlier versions. I am happy to add more resources to this section based on recommendations.</p>
<p>For anyone who has never programmed before, I recommend going through <a href="https://www.youtube.com/watch?v=KkMDCCdjyW8&amp;list=PL84A56BC7F4A1F852">Programming Methodology</a> by Prof. Mehran Sahami. While this playlist is on the older side, I think Prof. Sahami does a fantastic job of laying out the basics of programming in this course. This can safely be skipped if you already have some experience with programming.</p>
<p>Once you have whet your appetite for programming and basic computer science, <a href="https://www.youtube.com/watch?v=FIroM06V2MA&amp;list=PL-h0BZdG_K4kAmsfvAik-Za826pNbQd0d">CS106b</a> is a good segue into the world of algorithms and abstractions. If you are already familiar with most of the basic algorithms and abstractions listed above feel free to skip this course as well. If you think your algorithms and abstraction skills may have gotten a little rusty take a look at the <a href="https://web.stanford.edu/class/archive/cs/cs106b/cs106b.1214/">slides</a> from the version of the course taught by Keith Schwarz.</p>
<p>Now that you have had some experience with programming and learning how to abstract some of the basic ideas from code into general algorithms, it’s important to learn some of the most fundamental algorithms and algorthimic analysis techniques in computer science. To cover this material there are several fantastic resources. - <a href="https://www.youtube.com/watch?v=hbJMUzZtJgk&amp;list=PLyhSTP3Z5_mZ8krUa2JsvL7V755ogHgkK">CS161</a> by Prof. Tim Roughgarden is a great introduction to many of the most important algorithmic techniques you will need to know. - An equivalent alternative to the CS161 lectures, is a multi-part course by Prof. Roughgarden on Coursera split into <a href="https://www.coursera.org/learn/algorithms-divide-conquer">part 1 (divide and conquer)</a>, <a href="https://www.coursera.org/learn/algorithms-graphs-data-structures">part 2 (graph search, shortest paths)</a>, <a href="https://www.coursera.org/learn/algorithms-greedy">part 3 (greedy algorithms)</a>, and <a href="https://www.coursera.org/learn/algorithms-npcomplete">part 4 (NP-completeness)</a> also taught by Prof. Roughgarden. If you prefer, <a href="https://www.youtube.com/playlist?list=PLXFMmlk03Dt7Q0xr1PIAriY5623cKiH7V">playlist 1</a> and <a href="https://www.youtube.com/playlist?list=PLXFMmlk03Dt5EMI2s2WQBsLsZl7A5HEK6">playlist 2</a> are the corresponding YouTube playlists. I recommend this option over the CS161 lectures because of the audio quality. - A course series on introductory algorithms from MIT that covers a little more than CS161 (<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2020/index.htm">6.006</a> and <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/index.htm">6.046</a>). I preferred the dynamic programming lectures in this series.</p>
<p>Regardless of whether you choose to pursue theoretical or applied research, you will inevitably come across some literature that requires you to have a working systems background. The modern revolution in machine learning was at least in some part a consequence of the incredible advances we have made in hardware technology. Understanding the relationship between hardware and algorithms therefore becomes critical to an appreciation of modern machine learning.</p>
<p>For this basic foundation in systems, I would recommend <a href="https://www.youtube.com/playlist?list=PL9D558D49CA734A02">CS107</a> (for an updated version see <a href="https://www.youtube.com/playlist?list=PLoCMsyE1cvdWivlV-39KKsBKUX-4DvraN">this playlist</a>) and <a href="https://www.youtube.com/playlist?list=PLu77E6J7s6Ko3Ft4XcOX1yKW6iX3eEFqS">CS110</a>. If you already feel comfortable with systems but want a recap, I would recommend going through this short series by <a href="https://www.youtube.com/channel/UCseUQK4kC3x2x543nHtGpzw">Brian Will</a>: - <a href="https://www.youtube.com/watch?v=9-KUm9YpPm0">Hardware Basics Video</a>, - <a href="https://www.youtube.com/watch?v=9GDX-IyZ_C8">Operating Systems Video</a>, - <a href="https://youtu.be/xHu7qI1gDPA">Unix System Calls 1</a>, <a href="https://youtu.be/2DrjQBL5FMU">Unix System Calls 2</a>, and - <a href="https://www.youtube.com/playlist?list=PLFAC320731F539902">This playlist on Unix terminals and shells</a>.</p>
<h1 id="intermediate-math">Intermediate Math</h1>
<p>Now we come to some of the more advanced topics in math that will almost definitely be used either directly or indirectly in any contemporary machine learning research you may come across.</p>
<h2 id="linear-algebra">Linear Algebra</h2>
<ul>
<li><p>Vector space (V,+,·) over real or complex field (CANI-ADDU),</p>
<ul>
<li><p>A vector is an element of a vector space,</p></li>
<li><p>Intuitive vector space is euclidean space (isomorphic to cartesian power of real numbers) with vectors as arrows, sum using parallelogram rule (element-wise sum), and s-multiplication by scaling arrow (scale all elements),</p></li>
<li><p>Non-intuitive abstract vector space is the set of polynomials of fixed degree with natural addition and scaling,</p></li>
</ul></li>
<li><p>Vector subspace,</p></li>
<li><p>Linear combinations,</p></li>
<li><p>Linear maps,</p>
<ul>
<li><p>f(av + bw) = af(v) + bf(w),</p></li>
<li><p>Homomorphism between vector spaces,</p></li>
<li><p>Intuitively, keeps grid lines parallel and evenly spaced. Also doesn’t move origin, (rotation, reflection, scaling, skewing),</p></li>
</ul></li>
<li><p>Compositions of linear maps are also linear maps,</p></li>
<li><p>Set of linear maps between two vector spaces forms a vector space,</p></li>
<li><p>Dual vector space to vector space V is the set of linear maps from V to the real number line,</p>
<ul>
<li><p>A covector is an element of the dual vector space,</p></li>
<li><p>The dual vector space to the dual vector space to V is V when V is finite dimensional,</p></li>
</ul></li>
<li><p>Inner products (linearity in first argument, conjugate symmetry, positive definiteness),</p>
<ul>
<li><p>Cauchy-Schwarz inequality,</p></li>
<li><p>Projection,</p></li>
<li><p>Angle definition,</p></li>
<li><p>Orthogonal vectors,</p></li>
<li><p>Induced norm (absolute homogeneity, positive definiteness, triangle inequality) -&gt; induced metric/distance,</p></li>
</ul></li>
<li><p>Hamel-basis (finite dimensional basis),</p>
<ul>
<li><p>Can represent a vector in uncountably many ways,</p></li>
<li><p>Linear independence (can form one of the vectors using linear combinations of the others),</p></li>
<li><p>Linear span (set of all linear combinations of the vectors),</p></li>
<li><p>Hamel-basis is a finite subset of the vector space that is linearly independent and spans the entire space,</p></li>
<li><p>The dimension of a vector space V is given by the cardinality of a basis on V (well-defined),</p></li>
<li><p>The dual basis is a set of covectors where applying the the ith dual basis element to the ith basis element gives 1, otherwise 0,</p></li>
<li><p>Standard/canonical basis for euclidean space,</p></li>
</ul></li>
<li><p>Matrices are linear maps expressed in a grid form based on the chosen basis,</p>
<ul>
<li><p>Square and rectangle matrices,</p></li>
<li><p>Zero matrix,</p></li>
<li><p>Identity matrix (kronecker delta),</p></li>
</ul></li>
<li><p>Geometry of matrices (based on euclidean space),</p></li>
<li><p>Systems of linear equations,</p></li>
<li><p>Matrix transpose (AB)^T = B^T A^T,</p></li>
<li><p>Vectors and covectors as matrices,</p></li>
<li><p>Column space (vector space) and row space (dual space),</p></li>
<li><p>Euclidean inner/dot product &lt;v,w&gt; = v^T w,</p>
<ul>
<li><p>Induced euclidean/l2 norm,</p></li>
<li><p>Induced euclidean/l2 distance,</p></li>
</ul></li>
<li><p>Vector norms</p>
<ul>
<li><p>lp norms,</p></li>
<li><p>l1 norm, l_{} norm,</p></li>
<li><p>Unit ball,</p></li>
<li><p>Equivalence of vector norms (all norms are equivalent in euclidean space),</p></li>
</ul></li>
<li><p>Matrix multiplication (composition of linear maps),</p>
<ul>
<li><p>Matrix-vector product as linear combination of row/column space,</p></li>
<li><p>Associativity and distributivity,</p></li>
<li><p>Non-commutativity,</p></li>
</ul></li>
<li><p>Rank and null space of a matrix,</p></li>
<li><p>Determinant (volume of a parallelepiped),</p></li>
<li><p>Matrix inverse,</p></li>
<li><p>Cramer’s rule,</p></li>
<li><p>Gaussian elimination,</p>
<ul>
<li><p>Diagonal matrices,</p></li>
<li><p>Elimination matrices,</p></li>
<li><p>Permutation matrices,</p></li>
<li><p>Triangular matrices,</p></li>
<li><p>LU factorization (with pivots QLUP),</p></li>
</ul></li>
<li><p>Fundamental theorem of linear algebra,</p></li>
<li><p>Over and under determined systems of equations,</p></li>
<li><p>Linear least squares (normal equations),</p>
<ul>
<li><p>Gram matrix,</p></li>
<li><p>Symmetric matrices (antisymmetric matrices),</p></li>
<li><p>Quadratic forms,</p></li>
<li><p>Positive/negative (semi)-definite matrices,</p></li>
</ul></li>
<li><p>Trace of a matrix,</p></li>
<li><p>Cholesky decomposition of a symmetric, positive definite matrix,</p></li>
<li><p>Orthonormal/orthogonal matrices (Gram-Schmidt),</p></li>
<li><p>Change of basis by matrix multiplication,</p></li>
<li><p>Eigenvalues and Eigenvectors,</p>
<ul>
<li><p>Characteristic polynomial,</p></li>
<li><p>Linear independence of eigenvectors,</p></li>
<li><p>Left and right eigenvectors,</p></li>
<li><p>Variational approach,</p></li>
<li><p>Rayleigh quotient,</p></li>
<li><p>Trace is sum of eigenvalues,</p></li>
<li><p>Determinant is product of eigenvalues,</p></li>
</ul></li>
<li><p>Spectrum and spectral radius,</p></li>
<li><p>Spectral theorem (hermitian matrices),</p>
<ul>
<li><p>Real eigenvalues,</p></li>
<li><p>Orthogonal eigenvectors,</p></li>
</ul></li>
<li><p>Non-defective or diagonalizable matrices,</p></li>
<li><p>Similar matrices,</p></li>
<li><p>Eigendecomposition,</p></li>
<li><p>Singular value decomposition,</p>
<ul>
<li><p>Covariance matrix,</p></li>
<li><p>Singular values,</p></li>
<li><p>Left and right singular vectors,</p></li>
<li><p>Moore-penrose pseudoinverse,</p></li>
<li><p>weighted sum of outer products,</p></li>
<li><p>truncated SVD,</p></li>
</ul></li>
<li><p>Matrix norms,</p>
<ul>
<li><p>Submultiplicativity,</p></li>
<li><p>Frobenius norm (l2 norm of singular values),</p></li>
<li><p>Induced/operator matrix-p norms,</p></li>
<li><p>Spectral/matrix-2 norm (l_{} norm of singular values),</p></li>
<li><p>Matrix-1 norm (maximum l1 norm of the columns),</p></li>
<li><p>Matrix-norm (maximum l1 norm of the rows),</p></li>
<li><p>Nuclear/trace norm (l1 norm of singular values),</p></li>
</ul></li>
<li><p>Linear dimensionality reduction (principal components analysis),</p></li>
<li><p>(r,s)-Tensors (multilinear <span>[</span>linear in every entry<span>]</span> map from cartesian product of r covectors and s vectors to reals),</p>
<ul>
<li><p>Vectors are (1,0)-tensors for finite dimensional vector spaces,</p></li>
<li><p>Covectors are (0,1)-tensors,</p></li>
<li><p>Linear maps (matrices) are (1,1)-tensors,</p></li>
<li><p>Inner products (also matrices) are (0,2)-tensors,</p></li>
</ul></li>
<li><p>Components/coordinates of tensors,</p>
<ul>
<li><p>Plug in the basis and dual basis elements to get all components/coordinates of the tensor,</p></li>
</ul></li>
<li><p>Kronecker/tensor product (relation to outer product), and</p></li>
<li><p>Einstein notation (upper is up to down, lower is left to right).</p></li>
</ul>
<h2 id="real-analysis">Real Analysis</h2>
<p>TODO @yashsavani: Add details</p>
<h2 id="multivariable-calculus-using-analysis">Multivariable Calculus Using Analysis</h2>
<p>TODO @yashsavani: Add details</p>
<h2 id="probability-theory-measure-theory-and-stochastic-processes">Probability Theory (Measure Theory and Stochastic Processes)</h2>
<p>TODO @yashsavani: Add details</p>
<h2 id="statistics">Statistics</h2>
<p>TODO @yashsavani: Add details</p>
<p>Linear algebra is the foundation of almost all the math needed for machine learning. As my advisor says, “you can’t ever know enough linear algebra”. To develop a strong intuition for this material, I would first recommend going through <a href="https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">this excellent playlist</a> by <a href="https://www.3blue1brown.com/">3Blue1Brown</a> (Grant Sanderson). If you have never seen anything from his channel, you are in for a treat! Grant’s expositions on math concepts are mesmerizing. Once you have developed some of the intuition for linear algebra, I would recommend going through <a href="https://youtube.com/playlist?list=PLE7DDD91010BC51F8">this playlist</a> by the esteemed Prof. Gilbert Strang from MIT for a more thorough coverage of the material.</p>
<p>Once you have a strong core, one of the most important math topics to cover is real analysis and multivariable calculus. Most of the theoretical foundation in machine learning relies on analyzing the convergence of processes in the limit. Real analysis is a tough topic, so it helps to have multiple sources to refer to if you get stuck. Some of the resources I can recommend are - <a href="https://amazon.com/Analysis-II-Third-Readings-Mathematics/dp/9380250657">Analysis II</a> by the incredible Prof. Terrence Tao. - <a href="https://www.youtube.com/playlist?list=PLBh2i93oe2quABbNq4I_-hyjhW8eOdgrO">A playlist on Real Analysis</a> by <a href="https://www.youtube.com/c/brightsideofmaths">The Bright Side of Mathematics</a>. - <a href="https://www.youtube.com/watch?v=wCZ1VEmVjVo">This video on the Jacobian</a>. - <a href="https://www.youtube.com/playlist?list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7">This course on vector calculus</a> - <a href="https://www.youtube.com/playlist?list=PLHXZ9OQGMqxc_CvEy7xBKRQr6I214QJcd">Another course on vector calculus</a>. - <a href="https://www.youtube.com/playlist?list=PL04BA7A9EB907EDAF">Math 131</a> at Harvey Mudd College.</p>
<p>For a course in measure theory, with a specific interest in probability theory I recommend <a href="https://statweb.stanford.edu/~adembo/stat-310b/lnotes.PLE7DDD91010BC51F8">these notes</a> by Prof. Amire Dembo. Another resource is <a href="https://www.youtube.com/playlist?list=PLBh2i93oe2qswFOC98oSFc37-0f4S3D4z">this playlist</a> on probability theory by <a href="https://www.youtube.com/c/brightsideofmaths">The Bright Side of Mathematics</a>. Yet another great resource is a <a href="https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4">Probability Primer</a> by the mathematical monk.</p>
<p>For a background in statistics I recommend the <a href="http://stat.cmu.edu/~siva/705/main.html">36-705 lecture notes</a> By Prof. Sivaraman Balakrishnan.</p>
<p>These are some resources I have been recommended, but not had the time to go through and review myself. - https://www.youtube.com/playlist?list=PL05umP7R6ij1a6KdEy8PVE9zoCv6SlHRS - https://www.youtube.com/playlist?list=PL05umP7R6ij0Gw5SLIrOA1dMYScCx4oXT - https://www.youtube.com/playlist?list=PLwJRxp3blEvaxmHgI2iOzNP6KGLSyd4dz</p>
<h1 id="numerical-methods-and-optimization">Numerical Methods and Optimization</h1>
<ul>
<li><p>Matrix factorization (LU, Cholesky, QR, SVD),</p></li>
<li><p>Eigenproblems (power iteration, shifting, deflation, QR Iteration, Krylov subspace methods),</p></li>
<li><p>Fast Fourier Transform,</p></li>
<li><p>Varational methods and normal equations,</p></li>
<li><p>Sensitivity analysis,</p></li>
<li><p>Fixed-point iteration,</p></li>
<li><p>Newton’s method,</p></li>
<li><p>BFGS,</p></li>
<li><p>Automatic differentiation,</p></li>
<li><p>Gradient descent,</p></li>
<li><p>Conjugate gradient descent,</p></li>
<li><p>Proximal gradient descent,</p></li>
<li><p>Stochastic gradient descent,</p></li>
<li><p>Convexity,</p></li>
<li><p>Duality,</p></li>
<li><p>Karush Kuhn Tucker conditions,</p></li>
<li><p>Convex problems (LP, QP, SDP, SOCP),</p></li>
<li><p>Momentum,</p></li>
<li><p>Frank Wolfe methods, and</p></li>
<li><p>Mirror descent.</p></li>
</ul>
<p>Here are some resources</p>
<ul>
<li><p><a href="https://www.youtube.com/playlist?list=PLHrg69yaUAPeiLEsa-1KauSe2HaA0Wf6I">Numerical Algorithms playlist</a> with accompanying <a href="https://people.csail.mit.edu/jsolomon/share/book/numerical_book.PLE7DDD91010BC51F8">textbook</a></p></li>
<li><p><a href="https://youtube.com/playlist?list=PLRPU00LaonXQ27RBcq6jFJnyIbGw5azOI">Convex optimization course</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLMrJAkhIeNNRjxJ_sMtJ02geqw_-vuB7O">Linear algebra playlist</a></p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLMrJAkhIeNNRTVrHYDfjNyqzZ6Q6rsTyf">Scientific computing course</a></p></li>
<li><p><a href="https://www.cs.utexas.edu/users/flame/laff/alaff-beta/ALAFF.html">Numerical Linear Algebra online course</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLAPSKVSdi0oZPbS-UD_kwT4ePZQx_CiME">Convex optimization summer school</a></p></li>
</ul>
<h1 id="core-ml">Core ML</h1>
<ul>
<li><p>Perceptron (XOR),</p></li>
<li><p>SVM (Slater),</p></li>
<li><p>Kernels (Representer, Mercer),</p></li>
<li><p>Realizable PAC,</p></li>
<li><p>VC dimension,</p></li>
<li><p>Sauer’s lemma,</p></li>
<li><p>No Free Lunch Theorem,</p></li>
<li><p>Agnostic PAC,</p></li>
<li><p>Rademacher Complexity,</p></li>
<li><p>Fundamental theorem of statistical learning,</p></li>
<li><p>Bias-variance tradeoff,</p></li>
<li><p>double descent,</p></li>
<li><p>Approximation algorithms,</p></li>
<li><p>Clustering,</p></li>
<li><p>Neural networks,</p></li>
<li><p>Ensemble methods (bagging, boosting),</p></li>
<li><p>Probabilistic graphical models,</p></li>
<li><p>Multi-armed bandits, and</p></li>
<li><p>Markov decision processes.</p></li>
</ul>
<p>Note: no reinforcement learning here.</p>
<ul>
<li><p>The standard textbook <a href="https://cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">Understanding Machine Learning From Theory to Algorithms</a> by Prof. Shai Shalev-Shwartz and Prof. Shai Ben-David and the accompanying <a href="https://www.youtube.com/playlist?list=PLPW2keNyw-usgvmR7FTQ3ZRjfLs5jT4BO">lecture playlist</a></p></li>
<li><p>An overview of all the material and a good review if you are already familiar with these topics <a href="https://www.youtube.com/playlist?list=PLie7a1OUTSagZB9mFZnVBgsNfBtcUGJWB">EPFL playlist</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC">A course in statistical machine learning</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA">A more classical approach of machine learning theory</a>.</p></li>
</ul>
<p>Recommended but not personally reviewed. - https://www.youtube.com/playlist?list=PLAPSKVSdi0oac6hwCklK7pddglecmcAno - https://www.youtube.com/playlist?list=PLTPQEx-31JXhguCush5J7OGnEORofoCW9</p>
<h1 id="seminal-research-in-ml">Seminal Research in ML</h1>
<ul>
<li><p>Computer vision</p>
<ul>
<li><p>classification,</p></li>
<li><p>segmentation,</p></li>
<li><p>OCR,</p></li>
<li><p>optical flow, and</p></li>
<li><p>reconstruction</p></li>
</ul></li>
<li><p>Natural language processing</p>
<ul>
<li><p>syntax,</p></li>
<li><p>speech,</p></li>
<li><p>semantics,</p></li>
<li><p>translation,</p></li>
<li><p>QA,</p></li>
<li><p>retrieval</p></li>
</ul></li>
<li><p>Neural networks as universal function approximators,</p></li>
<li><p>Backpropagation,</p></li>
<li><p>batch normalization,</p></li>
<li><p>Dropout,</p></li>
<li><p>Momentum,</p></li>
<li><p>NTK,</p></li>
<li><p>CNN,</p></li>
<li><p>ResNet,</p></li>
<li><p>LSTM,</p></li>
<li><p>Language models,</p></li>
<li><p>Transformers,</p></li>
<li><p>VAE,</p></li>
<li><p>WGAN,</p></li>
<li><p>NAS.</p></li>
</ul>
<p>Some resources are: - <a href="https://deeplearningbook.org">The Deep Learning Book</a>. - <a href="https://github.com/terryum/awesome-deep-learning-papers">Repository of seminal deep learning papers</a> - <a href="https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI">NYU Deep Learning course</a>. - <a href="http://www.cs.umd.edu/class/fall2020/cmsc828W/">Foundations of Deep Learning course</a> from University of Maryland, College Park.</p>
<h1 id="tools">Tools</h1>
<ul>
<li><p>Python or Julia with conda,</p></li>
<li><p>Numerical libraries (numpy, PyTorch, JAX),</p></li>
<li><p>Data tools (SQL, NoSQL, redis, pandas, spark, S3),</p></li>
<li><p>System tools (ssh, MPI, docker, kubernetes),</p></li>
<li><p>Visualization tools (matplotlib, seaborn, PCA, t-SNE, UMAP),</p></li>
<li><p>statistical analysis tools (scipy, statsmodel), and</p></li>
<li><p>Notebooks (Jupyter, Pluto).</p></li>
</ul>
<p>Some resources are: - <a href="https://computationalthinking.mit.edu/Spring21/semesters/">Introduction to Computational Thinking</a> A course on Julia at MIT.</p>
<h1 id="additional-topics">Additional Topics</h1>
<ul>
<li><p>Optimal transport,</p></li>
<li><p>Algebraic/differential topology/geometry,</p></li>
<li><p>Information theory,</p></li>
<li><p>Game theory,</p></li>
<li><p>Statistical mechanics,</p></li>
<li><p>Psychology,</p></li>
<li><p>Cognitive science,</p></li>
<li><p>Graphics,</p></li>
<li><p>Hardware architecture,</p></li>
<li><p>Automata theory,</p></li>
<li><p>Type theory,</p></li>
<li><p>Category theory,</p></li>
<li><p><a href="https://www.youtube.com/c/braintruffle/videos">Computational Fluid Dynamics</a> (a visual masterpiece) and also introduces some important ideas about information reduction for computational feasibility.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLu7cY2CPiRjVY-VaUZ69bXHZr5QslKbzo">Intution for General Relativity</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLowKtXNTBypGqImE405J2565dvjafglHU">Breadboard computer</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLEGCF-WLh2RJh2yDxlJJjnKswWdoO8gAc">Advanced Algorithms Stanford CS261</a> by Prof. Tim Roughgarden.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PL2SOU6wwxB0uP4rJgf5ayhHWgw7akUWSf">Advanced Algorithms Harvard COMPSCI 224</a> by Prof. Jelani Nelson.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLFeEvEPtX_0S6vxxiiNPrJbLu9aK1UVC">General Relativity and more generally differential topology</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLMrJAkhIeNNR2W2sPWsYxfrxcASrUt_9j">Engineering Mathematics</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLo4jXE-LdDTTIIIRwqK35CbFJieSJEcVR">Functional Analysis</a>.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLAPSKVSdi0obG1b3w4k41JMLFbyBJS5AQ">Five Miracles of Mirror Descent</a>.</p></li>
</ul>
<h1 id="sec:acknowledgements">Acknowledgements</h1>
<p>I want to thank <a href="https://github.com/joshcho">Josh Cho</a> for giving me advice on this repository.</p>
</body>
</html>
