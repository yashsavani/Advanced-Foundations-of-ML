\hypertarget{intermediate-math}{%
\section{Intermediate Math}\label{intermediate-math}}

Now we come to some of the more advanced topics in math that will almost
definitely be used either directly or indirectly in any contemporary
machine learning research you may come across.

\hypertarget{linear-algebra}{%
\subsection{Linear Algebra}\label{linear-algebra}}

\begin{itemize}
\tightlist
\item
  Vector space (V,+,Â·) over real or complex field (CANI-ADDU),

  \begin{itemize}
  \tightlist
  \item
    A vector is an element of a vector space,
  \item
    Intuitive vector space is euclidean space (isomorphic to cartesian
    power of real numbers) with vectors as arrows, sum using
    parallelogram rule (element-wise sum), and s-multiplication by
    scaling arrow (scale all elements),
  \item
    Non-intuitive abstract vector space is the set of polynomials of
    fixed degree with natural addition and scaling,
  \end{itemize}
\item
  Vector subspace,
\item
  Linear combinations,
\item
  Linear maps,

  \begin{itemize}
  \tightlist
  \item
    f(av + bw) = af(v) + bf(w),
  \item
    Homomorphism between vector spaces,
  \item
    Intuitively, keeps grid lines parallel and evenly spaced. Also
    doesn't move origin, (rotation, reflection, scaling, skewing),
  \end{itemize}
\item
  Compositions of linear maps are also linear maps,
\item
  Set of linear maps between two vector spaces forms a vector space,
\item
  Dual vector space to vector space V is the set of linear maps from V
  to the real number line,

  \begin{itemize}
  \tightlist
  \item
    A covector is an element of the dual vector space,
  \item
    The dual vector space to the dual vector space to V is V when V is
    finite dimensional,
  \end{itemize}
\item
  Inner products (linearity in first argument, conjugate symmetry,
  positive definiteness),

  \begin{itemize}
  \tightlist
  \item
    Cauchy-Schwarz inequality,
  \item
    Projection,
  \item
    Angle definition,
  \item
    Orthogonal vectors,
  \item
    Induced norm (absolute homogeneity, positive definiteness, triangle
    inequality) -\textgreater{} induced metric/distance,
  \end{itemize}
\item
  Hamel-basis (finite dimensional basis),

  \begin{itemize}
  \tightlist
  \item
    Can represent a vector in uncountably many ways,
  \item
    Linear independence (can form one of the vectors using linear
    combinations of the others),
  \item
    Linear span (set of all linear combinations of the vectors),
  \item
    Hamel-basis is a finite subset of the vector space that is linearly
    independent and spans the entire space,
  \item
    The dimension of a vector space V is given by the cardinality of a
    basis on V (well-defined),
  \item
    The dual basis is a set of covectors where applying the the ith dual
    basis element to the ith basis element gives 1, otherwise 0,
  \item
    Standard/canonical basis for euclidean space,
  \end{itemize}
\item
  Matrices are linear maps expressed in a grid form based on the chosen
  basis,

  \begin{itemize}
  \tightlist
  \item
    Square and rectangle matrices,
  \item
    Zero matrix,
  \item
    Identity matrix (kronecker delta),
  \end{itemize}
\item
  Geometry of matrices (based on euclidean space),
\item
  Systems of linear equations,
\item
  Matrix transpose (AB)\^{}T = B\^{}T A\^{}T,
\item
  Vectors and covectors as matrices,
\item
  Column space (vector space) and row space (dual space),
\item
  Euclidean inner/dot product \textless v,w\textgreater{} = v\^{}T w,

  \begin{itemize}
  \tightlist
  \item
    Induced euclidean/l2 norm,
  \item
    Induced euclidean/l2 distance,
  \end{itemize}
\item
  Vector norms

  \begin{itemize}
  \tightlist
  \item
    lp norms,
  \item
    l1 norm, \(l_{\infty}\) norm,
  \item
    Unit ball,
  \item
    Equivalence of vector norms (all norms are equivalent in euclidean
    space),
  \end{itemize}
\item
  Matrix multiplication (composition of linear maps),

  \begin{itemize}
  \tightlist
  \item
    Matrix-vector product as linear combination of row/column space,
  \item
    Associativity and distributivity,
  \item
    Non-commutativity,
  \end{itemize}
\item
  Rank and null space of a matrix,
\item
  Determinant (volume of a parallelepiped),
\item
  Matrix inverse,
\item
  Cramer's rule,
\item
  Gaussian elimination,

  \begin{itemize}
  \tightlist
  \item
    Diagonal matrices,
  \item
    Elimination matrices,
  \item
    Permutation matrices,
  \item
    Triangular matrices,
  \item
    LU factorization (with pivots QLUP),
  \end{itemize}
\item
  Fundamental theorem of linear algebra,
\item
  Over and under determined systems of equations,
\item
  Linear least squares (normal equations),

  \begin{itemize}
  \tightlist
  \item
    Gram matrix,
  \item
    Symmetric matrices (antisymmetric matrices),
  \item
    Quadratic forms,
  \item
    Positive/negative (semi)-definite matrices,
  \end{itemize}
\item
  Trace of a matrix,
\item
  Cholesky decomposition of a symmetric, positive definite matrix,
\item
  Orthonormal/orthogonal matrices (Gram-Schmidt),
\item
  Change of basis by matrix multiplication,
\item
  Eigenvalues and Eigenvectors,

  \begin{itemize}
  \tightlist
  \item
    Characteristic polynomial,
  \item
    Linear independence of eigenvectors,
  \item
    Left and right eigenvectors,
  \item
    Variational approach,
  \item
    Rayleigh quotient,
  \item
    Trace is sum of eigenvalues,
  \item
    Determinant is product of eigenvalues,
  \end{itemize}
\item
  Spectrum and spectral radius,
\item
  Spectral theorem (hermitian matrices),

  \begin{itemize}
  \tightlist
  \item
    Real eigenvalues,
  \item
    Orthogonal eigenvectors,
  \end{itemize}
\item
  Non-defective or diagonalizable matrices,
\item
  Similar matrices,
\item
  Eigendecomposition,
\item
  Singular value decomposition,

  \begin{itemize}
  \tightlist
  \item
    Covariance matrix,
  \item
    Singular values,
  \item
    Left and right singular vectors,
  \item
    Moore-penrose pseudoinverse,
  \item
    weighted sum of outer products,
  \item
    truncated SVD,
  \end{itemize}
\item
  Matrix norms,

  \begin{itemize}
  \tightlist
  \item
    Submultiplicativity,
  \item
    Frobenius norm (l2 norm of singular values),
  \item
    Induced/operator matrix-p norms,
  \item
    Spectral/matrix-2 norm (\(l_{\infty}\) norm of singular values),
  \item
    Matrix-1 norm (maximum l1 norm of the columns),
  \item
    Matrix-\(\infty\) norm (maximum l1 norm of the rows),
  \item
    Nuclear/trace norm (l1 norm of singular values),
  \end{itemize}
\item
  Linear dimensionality reduction (principal components analysis),
\item
  (r,s)-Tensors (multilinear {[}linear in every entry{]} map from
  cartesian product of r covectors and s vectors to reals),

  \begin{itemize}
  \tightlist
  \item
    Vectors are (1,0)-tensors for finite dimensional vector spaces,
  \item
    Covectors are (0,1)-tensors,
  \item
    Linear maps (matrices) are (1,1)-tensors,
  \item
    Inner products (also matrices) are (0,2)-tensors,
  \end{itemize}
\item
  Components/coordinates of tensors,

  \begin{itemize}
  \tightlist
  \item
    Plug in the basis and dual basis elements to get all
    components/coordinates of the tensor,
  \end{itemize}
\item
  Kronecker/tensor product (relation to outer product), and
\item
  Einstein notation (upper is up to down, lower is left to right).
\end{itemize}

\hypertarget{real-analysis}{%
\subsection{Real Analysis}\label{real-analysis}}

TODO @yashsavani: Add details

\hypertarget{multivariable-calculus-using-analysis}{%
\subsection{Multivariable Calculus Using
Analysis}\label{multivariable-calculus-using-analysis}}

TODO @yashsavani: Add details

\hypertarget{probability-theory-measure-theory-and-stochastic-processes}{%
\subsection{Probability Theory (Measure Theory and Stochastic
Processes)}\label{probability-theory-measure-theory-and-stochastic-processes}}

TODO @yashsavani: Add details

\hypertarget{statistics}{%
\subsection{Statistics}\label{statistics}}

TODO @yashsavani: Add details

Linear algebra is the foundation of almost all the math needed for
machine learning. As my advisor says, ``you can't ever know enough
linear algebra''. To develop a strong intuition for this material, I
would first recommend going through
\href{https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab}{this
excellent playlist} by \href{https://www.3blue1brown.com/}{3Blue1Brown}
(Grant Sanderson). If you have never seen anything from his channel, you
are in for a treat! Grant's expositions on math concepts are
mesmerizing. Once you have developed some of the intuition for linear
algebra, I would recommend going through
\href{https://youtube.com/playlist?list=PLE7DDD91010BC51F8}{this
playlist} by the esteemed Prof.~Gilbert Strang from MIT for a more
thorough coverage of the material.

Once you have a strong core, one of the most important math topics to
cover is real analysis and multivariable calculus. Most of the
theoretical foundation in machine learning relies on analyzing the
convergence of processes in the limit. Real analysis is a tough topic,
so it helps to have multiple sources to refer to if you get stuck. Some
of the resources I can recommend are -
\href{https://amazon.com/Analysis-II-Third-Readings-Mathematics/dp/9380250657}{Analysis
II} by the incredible Prof.~Terrence Tao. -
\href{https://www.youtube.com/playlist?list=PLBh2i93oe2quABbNq4I_-hyjhW8eOdgrO}{A
playlist on Real Analysis} by
\href{https://www.youtube.com/c/brightsideofmaths}{The Bright Side of
Mathematics}. - \href{https://www.youtube.com/watch?v=wCZ1VEmVjVo}{This
video on the Jacobian}. -
\href{https://www.youtube.com/playlist?list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7}{This
course on vector calculus} -
\href{https://www.youtube.com/playlist?list=PLHXZ9OQGMqxc_CvEy7xBKRQr6I214QJcd}{Another
course on vector calculus}. -
\href{https://www.youtube.com/playlist?list=PL04BA7A9EB907EDAF}{Math
131} at Harvey Mudd College.

For a course in measure theory, with a specific interest in probability
theory I recommend
\href{https://statweb.stanford.edu/~adembo/stat-310b/lnotes.PLE7DDD91010BC51F8}{these
notes} by Prof.~Amire Dembo. Another resource is
\href{https://www.youtube.com/playlist?list=PLBh2i93oe2qswFOC98oSFc37-0f4S3D4z}{this
playlist} on probability theory by
\href{https://www.youtube.com/c/brightsideofmaths}{The Bright Side of
Mathematics}. Yet another great resource is a
\href{https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4}{Probability
Primer} by the mathematical monk.

For a background in statistics I recommend the
\href{http://stat.cmu.edu/~siva/705/main.html}{36-705 lecture notes} By
Prof.~Sivaraman Balakrishnan.

These are some resources I have been recommended, but not had the time
to go through and review myself. -
https://www.youtube.com/playlist?list=PL05umP7R6ij1a6KdEy8PVE9zoCv6SlHRS
-
https://www.youtube.com/playlist?list=PL05umP7R6ij0Gw5SLIrOA1dMYScCx4oXT
-
https://www.youtube.com/playlist?list=PLwJRxp3blEvaxmHgI2iOzNP6KGLSyd4dz
